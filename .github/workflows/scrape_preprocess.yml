name: Scrape_Preprocess

on:
  push:
    branches: ["main"]
    paths-ignore:
      - 'docs/**'            # or 'cite/**' if that's your actual folder
      - '.github/workflows/build_deploy.yml'  # editing deploy workflow shouldn't rerun pipeline
      - 'README.md'          # docs-only changes
  schedule:
    - cron: '0 18 * * *'     # still runs daily regardless of paths

jobs:
  update_data:
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.ref == 'refs/heads/main'
    permissions:
      contents: write
    env:
      LOG_LEVEL: WARNING
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0  # needed so we can pull/merge before pushing

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11.5'
          cache: 'pipenv'

      - name: Install pipenv
        run: curl https://raw.githubusercontent.com/pypa/pipenv/master/get-pipenv.py | python

      - name: Install Dependencies
        run: pipenv install

      - name: Install Playwright Browsers
        run: pipenv run playwright install

      # Clone private data repo and pull existing DB
      - name: Clone Private Data Repo
        run: |
          git clone https://${{ secrets.NLP_NEWS_SUMMARY_DATA }}@github.com/vsevolodnedora/nlp_news_summary_data.git data_repo
          mkdir -p database
          cp data_repo/database/scraped_posts.db database/scraped_posts.db
          cp data_repo/database/preprocessed_posts.db database/preprocessed_posts.db
          # if you also have tkg.db in the private repo, add:
          # cp data_repo/database/tkg.db database/tkg.db

      # Run scrapers and dump summary of the database into the public_view/
      - name: Scrape Publications from Publishers
        run: pipenv run python run_scrape.py all

      # Run preprocessor on the raw publication database and dump summary into public_view/
      - name: Preprocess Publications to clean the HTML and extract text
        run: pipenv run python run_preprocess.py all

      # Copy updated DB back into private repo and push
      - name: Push Updated DB to Private Repo
        run: |
          cp database/scraped_posts.db data_repo/database/scraped_posts.db
          cp database/preprocessed_posts.db data_repo/database/preprocessed_posts.db

          cd data_repo
          git config user.name 'Collector'
          git config user.email 'noreply@nedora.digital'
          git add database/scraped_posts.db
          git add database/preprocessed_posts.db
          git commit -m "Update databases from scrape & preprocess run at $(date -u)" || echo "No changes to commit in private repo"
          git push

      # Commit and push generated public_view files to this repo
      - name: Commit and Push public_view outputs
        run: |
          # Configure git for this repo
          git config user.name 'Collector'
          git config user.email 'noreply@nedora.digital'

          # Make sure output/public_view exists (avoid 'pathspec did not match' errors)
          mkdir -p output/public_view

          # Stage only the public_view output directory
          git add output/public_view || echo "No output/public_view directory to add"

          # If nothing staged, bail out early
          if git diff --cached --quiet; then
            echo "No changes in output/public_view to commit"
            exit 0
          fi

          # Pull remote changes first to avoid non-fast-forward push
          git pull --no-edit -X ours origin main || echo "No remote changes to merge or merge failed"

          # Commit
          git commit -m "Update output/public_view from scraper run at $(date -u)"

          # Push back to main
          git push origin HEAD:main