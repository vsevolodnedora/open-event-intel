name: Scrape_Preprocess

on:
  push:
    branches: ["main"]
    paths-ignore:
      - "docs/**"
      - ".github/workflows/build_deploy.yml"
      - "README.md"
  schedule:
    - cron: "0 18 * * *"
  workflow_dispatch:

concurrency:
  group: scrape-preprocess-main
  cancel-in-progress: false

jobs:
  scrape:
    name: Scrape publishers
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.ref == 'refs/heads/main'
    permissions:
      contents: read
    env:
      LOG_LEVEL: WARNING

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11.5"
          cache: "pipenv"

      - name: Install pipenv
        run: python -m pip install --upgrade pip pipenv

      - name: Install Dependencies
        run: pipenv install --deploy

      - name: Install Playwright Browsers
        run: pipenv run playwright install --with-deps

      - name: Checkout Private Data Repo
        uses: actions/checkout@v4
        with:
          repository: vsevolodnedora/nlp_news_summary_data
          token: ${{ secrets.NLP_NEWS_SUMMARY_DATA }}
          path: data_repo
          fetch-depth: 0

      - name: Prepare DBs and private prompts
        run: |
          mkdir -p database
          cp data_repo/database/scraped_posts.db database/scraped_posts.db
          cp data_repo/database/preprocessed_posts.db database/preprocessed_posts.db

          mkdir -p src/tkg
          cp -r data_repo/prompts_and_definitions src/tkg/prompts_and_definitions

      - name: Install this package (editable)
        run: pipenv run pip install -e .

      - name: Scrape Publications from Publishers
        run: pipenv run python -m open_event_intel.scraping.run_scrape \
          --source all \
          --db-path database/scraped_posts.db \
          --output-dir output/posts_raw/ \
          --metadata-dir output/public_view/

      - name: Remove private prompts_and_definitions
        if: always()
        run: rm -rf src/tkg/prompts_and_definitions

      - name: Upload DBs artifact (for downstream jobs)
        uses: actions/upload-artifact@v4
        with:
          name: dbs
          path: |
            database/scraped_posts.db
            database/preprocessed_posts.db

  preprocess:
    name: Preprocess + export for web
    runs-on: ubuntu-latest
    needs: scrape
    permissions:
      contents: read
    env:
      LOG_LEVEL: WARNING

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11.5"
          cache: "pipenv"

      - name: Install pipenv
        run: python -m pip install --upgrade pip pipenv

      - name: Install Dependencies
        run: pipenv install --deploy

      - name: Install Playwright Browsers
        run: pipenv run playwright install --with-deps

      - name: Install this package (editable)
        run: pipenv run pip install -e .

      - name: Download DBs artifact
        uses: actions/download-artifact@v4
        with:
          name: dbs
          path: database

      - name: Preprocess publications (clean HTML, extract text)
        run: pipenv run python -m open_event_intel.scraping.run_preprocessor \
          --source all \
          --source-db database/scraped_posts.db \
          --target-db database/preprocessed_posts.db \
          --output-dir output/posts_cleaned/ \
          --failed-dir output/failed_preprocess/ \
          --overwrite \
          --metadata-output output/public_view/ \
          --corruptions-file config/possible_corruptions.txt

      - name: Export pipeline data for website (docs/)
        run: pipenv run python -m open_event_intel.scraping.export_pipeline_data \
          --scraped-db database/scraped_posts.db \
          --preprocessed-db database/preprocessed_posts.db \
          --output-dir docs/scrape_data/ \
          --window-days 60

      - name: Upload updated DBs artifact
        uses: actions/upload-artifact@v4
        with:
          name: dbs-updated
          path: |
            database/scraped_posts.db
            database/preprocessed_posts.db

      - name: Upload docs artifact
        uses: actions/upload-artifact@v4
        with:
          name: docs-scrape-data
          path: docs/scrape_data

  push_private:
    name: Push DBs to private repo
    runs-on: ubuntu-latest
    needs: preprocess
    permissions:
      contents: read

    steps:
      - name: Download updated DBs artifact
        uses: actions/download-artifact@v4
        with:
          name: dbs-updated
          path: database

      - name: Checkout Private Data Repo
        uses: actions/checkout@v4
        with:
          repository: vsevolodnedora/nlp_news_summary_data
          token: ${{ secrets.NLP_NEWS_SUMMARY_DATA }}
          path: data_repo
          fetch-depth: 0

      - name: Copy DBs and push
        run: |
          cp database/scraped_posts.db data_repo/database/scraped_posts.db
          cp database/preprocessed_posts.db data_repo/database/preprocessed_posts.db

          cd data_repo
          git config user.name 'Collector'
          git config user.email 'noreply@nedora.digital'
          git add database/scraped_posts.db database/preprocessed_posts.db
          git commit -m "Update databases from scrape & preprocess run at $(date -u)" || echo "No changes to commit in private repo"
          git push

  publish_docs:
    name: Commit public docs/scrape_data to main
    runs-on: ubuntu-latest
    needs: preprocess
    permissions:
      contents: write

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Download docs artifact
        uses: actions/download-artifact@v4
        with:
          name: docs-scrape-data
          path: docs/scrape_data

      - name: Commit and push docs/scrape_data
        run: |
          git config user.name 'Collector'
          git config user.email 'noreply@nedora.digital'

          git add docs/scrape_data || echo "No docs/scrape_data directory to add"

          if git diff --cached --quiet; then
            echo "No changes in docs/scrape_data to commit"
            exit 0
          fi

          git pull --no-edit -X ours origin main || echo "No remote changes to merge or merge failed"
          git commit -m "Update docs/scrape_data from scraper run at $(date -u)"
          git push origin HEAD:main
