name: preprocess_posts

on:
  workflow_run:
    workflows: ["scrape_posts"]
    types: [completed]
    branches: [main]
  workflow_dispatch:

concurrency:
  # One run per branch at a time (main will serialize)
  group: scrape-preprocess-${{ github.ref_name }}
  cancel-in-progress: false

jobs:
  preprocess:
    name: Preprocess + export for web
    runs-on: ubuntu-latest

    if: >-
      ${{
        (github.event_name == 'workflow_run' && github.event.workflow_run.conclusion == 'success') ||
        (github.event_name == 'workflow_dispatch' && github.ref_name == 'main')
      }}

    permissions:
      contents: read

    env:
      LOG_LEVEL: WARNING

    steps:
      - name: Debug trigger context (handy while tuning)
        run: |
          echo "event_name=${{ github.event_name }}"
          echo "sha=${{ github.sha }}"
          echo "ref=${{ github.ref }}"
          echo "workflow_run_head_sha=${{ github.event.workflow_run.head_sha }}"
          echo "workflow_run_conclusion=${{ github.event.workflow_run.conclusion }}"

      - name: Checkout Repository (scrape commit if workflow_run, else current push)
        uses: actions/checkout@v4
        with:
          ref: ${{ (github.event_name == 'workflow_run' && github.event.workflow_run.head_sha) || github.sha }}
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11.5"
          cache: "pipenv"

      - name: Install pipenv
        run: python -m pip install --upgrade pip pipenv

      - name: Install Dependencies
        run: pipenv install --deploy

      - name: Install Playwright Browsers
        run: pipenv run playwright install --with-deps

      - name: Checkout Private Data Repo
        uses: actions/checkout@v4
        with:
          repository: vsevolodnedora/nlp_news_summary_data
          token: ${{ secrets.NLP_NEWS_SUMMARY_DATA }}
          path: data_repo
          fetch-depth: 0

      - name: Prepare DBs and private prompts
        run: |
          mkdir -p database
          cp data_repo/database/scraped_posts.db database/scraped_posts.db
          cp data_repo/database/preprocessed_posts.db database/preprocessed_posts.db

      - name: Install this package (editable)
        run: pipenv run pip install -e .

      # Run the main preprocessing pipeline
      - name: Preprocess publications (clean HTML, extract text)
        run: |
          pipenv run python -m open_event_intel.preprocessing.run_preprocessor \
            --source all \
            --source-db database/scraped_posts.db \
            --target-db database/preprocessed_posts.db \
            --output-dir output/posts_cleaned/ \
            --failed-dir output/failed_preprocess/ \
            --overwrite \
            --metadata-output output/public_view/ \
            --corruptions-file config/possible_corruptions.txt

      # Remove (normalized) duplicates so that later ETL does not have to
      - name: Preprocess publications (clean HTML, extract text)
        run: |
          pipenv run python -m open_event_intel.preprocessing.clean_normalized \
            --source-db database/preprocessed_posts.db \
            --apply \

      # Export Scraped and cleaned data into small public databases (for the webpage)
      - name: Export pipeline data for website (docs/)
        run: |
          pipenv run python -m open_event_intel.preprocessing.export_scrape_pipeline_data \
            --scraped-db database/scraped_posts.db \
            --preprocessed-db database/preprocessed_posts.db \
            --output-dir docs/scrape_data/ \
            --window-days 60

      - name: Upload preprocessed DB artifact (for push job)
        uses: actions/upload-artifact@v4
        with:
          name: preprocessed-db
          path: database/preprocessed_posts.db

      - name: Upload docs artifact
        uses: actions/upload-artifact@v4
        with:
          name: docs-scrape-data
          path: docs/scrape_data

  push_private:
    name: Push preprocessed_posts.db to private repo
    runs-on: ubuntu-latest
    needs: preprocess
    permissions:
      contents: read

    steps:
      - name: Download preprocessed DB artifact
        uses: actions/download-artifact@v4
        with:
          name: preprocessed-db
          path: database

      - name: Checkout Private Data Repo
        uses: actions/checkout@v4
        with:
          repository: vsevolodnedora/nlp_news_summary_data
          token: ${{ secrets.NLP_NEWS_SUMMARY_DATA }}
          path: data_repo
          fetch-depth: 0

      - name: Copy preprocessed DB and push
        run: |
          cp database/preprocessed_posts.db data_repo/database/preprocessed_posts.db

          cd data_repo
          git config user.name 'Collector'
          git config user.email 'noreply@nedora.digital'
          git add database/preprocessed_posts.db
          git commit -m "Update preprocessed_posts.db from preprocess run at $(date -u)" || echo "No changes to commit in private repo"
          git push

  publish_docs:
    name: Commit public docs/scrape_data to main
    runs-on: ubuntu-latest
    needs: push_private
    permissions:
      contents: write

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Download docs artifact
        uses: actions/download-artifact@v4
        with:
          name: docs-scrape-data
          path: docs/scrape_data

      - name: Commit and push docs/scrape_data
        run: |
          git config user.name 'Collector'
          git config user.email 'noreply@nedora.digital'

          git add docs/scrape_data || echo "No docs/scrape_data directory to add"

          if git diff --cached --quiet; then
            echo "No changes in docs/scrape_data to commit"
            exit 0
          fi

          git pull --no-edit -X ours origin main || echo "No remote changes to merge or merge failed"
          git commit -m "Update docs/scrape_data from scraper run at $(date -u)"
          git push origin HEAD:main
